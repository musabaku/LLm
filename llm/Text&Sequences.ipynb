{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for text and sequences\n",
    "\n",
    "This chapter explores deep-learning models that can process text (understood as sequences of word or sequences of characters), time series, and sequence data in general. The two fundamental deep-learning algorithms for sequence processing are recurrent neural networks and 1D convnets, the one-dimensional version of the 2D\n",
    "convnets that we covered in the previous chapters. We’ll discuss both of these\n",
    "approaches in this chapter.\n",
    "\n",
    "Applications of these algorithms include the following:\n",
    "- Document classification and time series classification, such as identifying the topic of an article or the author of a book\n",
    "- Timeseries comparisons, such as estimating how closely related two documents or two stock tickers are\n",
    "-  Sequence-to-sequence learning, such as decoding an English sentence into French\n",
    "- Sentiment analysis, such as classifying the sentiment of tweets or movie review as positive or negative\n",
    "- Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data\n",
    "\n",
    "This chapter’s examples focus on two narrow tasks: sentiment analysis on the IMDB\n",
    "dataset, a task we approached earlier in the book, and temperature forecasting. But\n",
    "the techniques demonstrated for these two tasks are relevant to all the applications\n",
    "just listed, and many more.\n",
    "\n",
    "## Working with text data\n",
    "Text is one of the most widespread forms of sequence data. It can be understood as\n",
    "either a sequence of characters or a sequence of words, but it’s most common to work\n",
    "at the level of words. The deep-learning sequence-processing models introduced in\n",
    "the following sections can use text to produce a basic form of natural-language understanding,\n",
    "sufficient for applications including document classification, sentiment\n",
    "analysis, author identification, and even question-answering (QA) (in a constrained\n",
    "context). Of course, keep in mind throughout this chapter that none of these deeplearning\n",
    "models truly understand text in a human sense; rather, these models can\n",
    "map the statistical structure of written language, which is sufficient to solve many simple\n",
    "textual tasks. Deep learning for natural-language processing is pattern recognition\n",
    "applied to words, sentences, and paragraphs, in much the same way that computer\n",
    "vision is pattern recognition applied to pixels.\n",
    "\n",
    "Like all other neural networks, deep-learning models don’t take as input raw text:\n",
    "they only work with numeric tensors. Vectorizing text is the process of transforming text\n",
    "into numeric tensors. This can be done in multiple ways:\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "\n",
    "N-grams are overlapping groups of multiple consecutive words or characters.\n",
    "Collectively, the different units into which you can break down text (words, characters,\n",
    "or n-grams) are called tokens, and breaking text into such tokens is called tokenization.\n",
    "All text-vectorization processes consist of applying some tokenization scheme and\n",
    "then associating numeric vectors with the generated tokens. These vectors, packed\n",
    "into sequence tensors, are fed into deep neural networks. There are multiple ways to\n",
    "associate a vector with a token. In this section, I’ll present two major ones: one-hot\n",
    "encoding of tokens, and token embedding (typically used exclusively for words, and called\n",
    "word embedding). The remainder of this section explains these techniques and shows\n",
    "how to use them to go from raw text to a Numpy tensor that you can send to a Keras\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx1.png\" alt=\"Drawing\" style=\"width:600px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding n-grams and bag-of-words\n",
    "Word n-grams are groups of N (or fewer) consecutive words that you can extract from\n",
    "a sentence. The same concept may also be applied to characters instead of words.\n",
    "\n",
    "Here’s a simple example. Consider the sentence “The cat sat on the mat.” It may be\n",
    "decomposed into the following set of 2-grams:\n",
    "\n",
    "    {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "    \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "It may also be decomposed into the following set of 3-grams:\n",
    "\n",
    "    {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\n",
    "    \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\",\n",
    "    \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "\n",
    "Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag\n",
    "here refers to the fact that you’re dealing with a set of tokens rather than a list or\n",
    "sequence: the tokens have no specific order. This family of tokenization methods is\n",
    "called bag-of-words.\n",
    "\n",
    "Because bag-of-words isn’t an order-preserving tokenization method (the tokens generated\n",
    "are understood as a set, not a sequence, and the general structure of the sentences\n",
    "is lost), it tends to be used in shallow language-processing models rather than\n",
    "in deep-learning models. Extracting n-grams is a form of feature engineering, and\n",
    "deep learning does away with this kind of rigid, brittle approach, replacing it with hierarchical\n",
    "feature learning. One-dimensional convnets and recurrent neural networks,\n",
    "introduced later in this chapter, are capable of learning representations for groups of\n",
    "words and characters without being explicitly told about the existence of such groups,\n",
    "by looking at continuous word or character sequences. For this reason, we won’t\n",
    "cover n-grams any further in this book. But do keep in mind that they’re a powerful,\n",
    "unavoidable feature-engineering tool when using lightweight, shallow text-processing\n",
    "models such as logistic regression and random forests.\n",
    "\n",
    "## One-hot encoding of words and characters\n",
    "One-hot encoding is the most common, most basic way to turn a token into a vector.\n",
    "You saw it in action in the initial IMDB and Reuters examples in chapter 3 (done with\n",
    "words, in that case). It consists of associating a unique integer index with every word\n",
    "and then turning this integer index i into a binary vector of size N (the size of the\n",
    "vocabulary); the vector is all zeros except for the i th entry, which is 1.\n",
    "Of course, one-hot encoding can be done at the character level, as well. To unambiguously\n",
    "drive home what one-hot encoding is and how to implement it, listings 6.1\n",
    "and 6.2 show two toy examples: one for words, the other for characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx2.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx3.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Keras has built-in utilities for doing one-hot encoding of text at the word level\n",
    "or character level, starting from raw text data. You should use these utilities, because\n",
    "they take care of a number of important features such as stripping special characters\n",
    "from strings and only taking into account the N most common words in your dataset (a\n",
    "common restriction, to avoid dealing with very large input vector spaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx4.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx5.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use\n",
    "when the number of unique tokens in your vocabulary is too large to handle explicitly.\n",
    "Instead of explicitly assigning an index to each word and keeping a reference of these\n",
    "indices in a dictionary, you can hash words into vectors of fixed size. This is typically\n",
    "done with a very lightweight hashing function. The main advantage of this method is\n",
    "that it does away with maintaining an explicit word index, which saves memory and\n",
    "allows online encoding of the data (you can generate token vectors right away, before\n",
    "you’ve seen all of the available data). The one drawback of this approach is that it’s\n",
    "susceptible to hash collisions: two different words may end up with the same hash, and\n",
    "subsequently any machine-learning model looking at these hashes won’t be able to tell\n",
    "the difference between these words. The likelihood of hash collisions decreases when\n",
    "the dimensionality of the hashing space is much larger than the total number of\n",
    "unique tokens being hashed.\n",
    "\n",
    "## Using word embeddings\n",
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Whereas the vectors obtained through one-hot\n",
    "encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same\n",
    "dimensionality as the number of words in the vocabulary), word embeddings are lowdimensional\n",
    "floating-point vectors (that is, dense vectors, as opposed to sparse vectors);\n",
    "see figure 6.2. Unlike the word vectors obtained via one-hot encoding, word\n",
    "embeddings are learned from data. It’s common to see word embeddings that are\n",
    "256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large\n",
    "vocabularies. On the other hand, one-hot encoding words generally leads to vectors\n",
    "that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx6.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to obtain word embeddings:\n",
    "- Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
    "- Load into your model word embeddings that were precomputed using a different machine-learning task than the one you’re trying to solve. These are called pretrained word embeddings.\n",
    "\n",
    "Let’s look at both.\n",
    "\n",
    "### LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER\n",
    "The simplest way to associate a dense vector with a word is to choose the vector at\n",
    "random. The problem with this approach is that the resulting embedding space has\n",
    "no structure: for instance, the words accurate and exact may end up with completely\n",
    "different embeddings, even though they’re interchangeable in most sentences. It’s\n",
    "difficult for a deep neural network to make sense of such a noisy, unstructured\n",
    "embedding space.\n",
    "\n",
    "To get a bit more abstract, the geometric relationships between word vectors\n",
    "should reflect the semantic relationships between these words. Word embeddings are\n",
    "meant to map human language into a geometric space. For instance, in a reasonable\n",
    "embedding space, you would expect synonyms to be embedded into similar word vectors;\n",
    "and in general, you would expect the geometric distance (such as L2 distance)\n",
    "between any two word vectors to relate to the semantic distance between the associated\n",
    "words (words meaning different things are embedded at points far away from\n",
    "each other, whereas related words are closer). In addition to distance, you may want\n",
    "specific directions in the embedding space to be meaningful. To make this clearer, let’s\n",
    "look at a concrete example.\n",
    "\n",
    "In figure 6.3, four words are embedded on a 2D plane:\n",
    "cat, dog, wolf, and tiger. With the vector representations we\n",
    "chose here, some semantic relationships between these\n",
    "words can be encoded as geometric transformations. For\n",
    "instance, the same vector allows us to go from cat to tiger\n",
    "and from dog to wolf : this vector could be interpreted as the\n",
    "“from pet to wild animal” vector. Similarly, another vector\n",
    "lets us go from dog to cat and from wolf to tiger, which could\n",
    "be interpreted as a “from canine to feline” vector.\n",
    "In real-world word-embedding spaces, common examples\n",
    "of meaningful geometric transformations are “gender”\n",
    "vectors and “plural” vectors. For instance, by adding a “female” vector to the vector\n",
    "“king,” we obtain the vector “queen.” By adding a “plural” vector, we obtain “kings.”\n",
    "Word-embedding spaces typically feature thousands of such interpretable and potentially\n",
    "useful vectors.\n",
    "\n",
    "Is there some ideal word-embedding space that would perfectly map human language\n",
    "and could be used for any natural-language-processing task? Possibly, but we\n",
    "have yet to compute anything of the sort. Also, there is no such a thing as human language—\n",
    "there are many different languages, and they aren’t isomorphic, because a language\n",
    "is the reflection of a specific culture and a specific context. But more\n",
    "pragmatically, what makes a good word-embedding space depends heavily on your task:\n",
    "the perfect word-embedding space for an English-language movie-review sentiment analysis\n",
    "model may look different from the perfect embedding space for an English language\n",
    "legal-document-classification model, because the importance of certain\n",
    "semantic relationships varies from task to task.\n",
    "\n",
    "It’s thus reasonable to learn a new embedding space with every new task. Fortunately,\n",
    "backpropagation makes this easy, and Keras makes it even easier. It’s about\n",
    "learning the weights of a layer: the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx7.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx8.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx9.png\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is best understood as a dictionary that maps integer indices\n",
    "(which stand for specific words) to dense vectors. It takes integers as input, it looks up\n",
    "these integers in an internal dictionary, and it returns the associated vectors. It’s effectively\n",
    "a dictionary lookup (see figure 6.4).\n",
    "\n",
    "The Embedding layer takes as input a 2D tensor of integers, of shape (samples,\n",
    "sequence_length), where each entry is a sequence of integers. It can embed\n",
    "sequences of variable lengths: for instance, you could feed into the Embedding layer in\n",
    "the previous example batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15). All sequences in a batch must\n",
    "have the same length, though (because you need to pack them into a single tensor),\n",
    "so sequences that are shorter than others should be padded with zeros, and sequences\n",
    "that are longer should be truncated.\n",
    "This layer returns a 3D floating-point tensor of shape (samples, sequence_\n",
    "length, embedding_dimensionality). Such a 3D tensor can then be processed by\n",
    "an RNN layer or a 1D convolution layer (both will be introduced in the following\n",
    "sections).\n",
    "\n",
    "When you instantiate an Embedding layer, its weights (its internal dictionary of\n",
    "token vectors) are initially random, just as with any other layer. During training, these\n",
    "word vectors are gradually adjusted via backpropagation, structuring the space into\n",
    "something the downstream model can exploit. Once fully trained, the embedding\n",
    "space will show a lot of structure—a kind of structure specialized for the specific problem\n",
    "for which you’re training your model.\n",
    "Let’s apply this idea to the IMDB movie-review sentiment-prediction task that\n",
    "you’re already familiar with. First, you’ll quickly prepare the data. You’ll restrict the\n",
    "movie reviews to the top 10,000 most common words (as you did the first time you\n",
    "worked with this dataset) and cut off the reviews after only 20 words. The network will\n",
    "learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the\n",
    "tensor to 2D, and train a single Dense layer on top for classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx10.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx11.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get to a validation accuracy of ~76%, which is pretty good considering that you’re\n",
    "only looking at the first 20 words in every review. But note that merely flattening the\n",
    "embedded sequences and training a single Dense layer on top leads to a model that\n",
    "treats each word in the input sequence separately, without considering inter-word\n",
    "relationships and sentence structure (for example, this model would likely treat both\n",
    "“this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s\n",
    "much better to add recurrent layers or 1D convolutional layers on top of the embedded\n",
    "sequences to learn features that take into account each sequence as a whole.\n",
    "That’s what we’ll focus on in the next few sections.\n",
    "\n",
    "### USING PRETRAINED WORD EMBEDDINGS\n",
    "Sometimes, you have so little training data available that you can’t use your data\n",
    "alone to learn an appropriate task-specific embedding of your vocabulary. What do\n",
    "you do then?\n",
    "\n",
    "Instead of learning word embeddings jointly with the problem you want to solve,\n",
    "you can load embedding vectors from a precomputed embedding space that you\n",
    "know is highly structured and exhibits useful properties—that captures generic\n",
    "aspects of language structure. The rationale behind using pretrained word embeddings\n",
    "in natural-language processing is much the same as for using pretrained convnets\n",
    "in image classification: you don’t have enough data available to learn truly\n",
    "powerful features on your own, but you expect the features that you need to be fairly\n",
    "generic—that is, common visual features or semantic features. In this case, it makes\n",
    "sense to reuse features learned on a different problem.\n",
    "\n",
    "Such word embeddings are generally computed using word-occurrence statistics\n",
    "(observations about what words co-occur in sentences or documents), using a variety of\n",
    "techniques, some involving neural networks, others not. The idea of a dense, lowdimensional\n",
    "embedding space for words, computed in an unsupervised way, was initially\n",
    "explored by Bengio et al. in the early 2000s,1 but it only started to take off in\n",
    "research and industry applications after the release of one of the most famous and successful\n",
    "word-embedding schemes: the Word2vec algorithm (https://code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec\n",
    "dimensions capture specific semantic properties, such as gender.\n",
    "There are various precomputed databases of word embeddings that you can download\n",
    "and use in a Keras Embedding layer. Word2vec is one of them. Another popular\n",
    "one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. This\n",
    "embedding technique is based on factorizing a matrix of word co-occurrence statistics.\n",
    "Its developers have made available precomputed embeddings for millions of\n",
    "English tokens, obtained from Wikipedia data and Common Crawl data.\n",
    "Let’s look at how you can get started using GloVe embeddings in a Keras model.\n",
    "The same method is valid for Word2vec embeddings or any other word-embedding\n",
    "database. You’ll also use this example to refresh the text-tokenization techniques\n",
    "introduced a few paragraphs ago: you’ll start from raw text and work your way up.\n",
    "\n",
    "## Putting it all together: from raw text to word embeddings\n",
    "You’ll use a model similar to the one we just went over: embedding sentences in\n",
    "sequences of vectors, flattening them, and training a Dense layer on top. But you’ll do\n",
    "so using pretrained word embeddings; and instead of using the pretokenized IMDB\n",
    "data packaged in Keras, you’ll start from scratch by downloading the original text data.\n",
    "\n",
    "### DOWNLOADING THE IMDB DATA AS RAW TEXT\n",
    "First, head to http://mng.bz/0tIo and download the raw IMDB dataset. Uncompress it.\n",
    "Now, let’s collect the individual training reviews into a list of strings, one string per\n",
    "review. You’ll also collect the review labels (positive/negative) into a labels list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx12.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZING THE DATA\n",
    "Let’s vectorize the text and prepare a training and validation split, using the concepts\n",
    "introduced earlier in this section. Because pretrained word embeddings are meant to\n",
    "be particularly useful on problems where little training data is available (otherwise,\n",
    "task-specific embeddings are likely to outperform them), we’ll add the following twist:\n",
    "restricting the training data to the first 200 samples. So you’ll learn to classify movie\n",
    "reviews after looking at just 200 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx13.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx14.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOADING THE GLOVE WORD EMBEDDINGS\n",
    "Go to https://nlp.stanford.edu/projects/glove, and download the precomputed\n",
    "embeddings from 2014 English Wikipedia. It’s an 822 MB zip file called glove.6B.zip,\n",
    "containing 100-dimensional embedding vectors for 400,000 words (or nonword\n",
    "tokens). Unzip it.\n",
    "\n",
    "### PREPROCESSING THE EMBEDDINGS\n",
    "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)\n",
    "to their vector representation (as number vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx15.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you’ll build an embedding matrix that you can load into an Embedding layer. It\n",
    "must be a matrix of shape (max_words, embedding_dim), where each entry i contains\n",
    "the embedding_dim-dimensional vector for the word of index i in the reference word\n",
    "index (built during tokenization). Note that index 0 isn’t supposed to stand for any\n",
    "word or token—it’s a placeholder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx17.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING A MODEL\n",
    "You’ll use the same model architecture as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx18.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING THE GLOVE EMBEDDINGS IN THE MODEL\n",
    "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is\n",
    "the word vector meant to be associated with index i. Simple enough. Load the GloVe\n",
    "matrix you prepared into the Embedding layer, the first layer in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx19.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you’ll freeze the Embedding layer (set its trainable attribute to False),\n",
    "following the same rationale you’re already familiar with in the context of pretrained\n",
    "convnet features: when parts of a model are pretrained (like your Embedding layer)\n",
    "and parts are randomly initialized (like your classifier), the pretrained parts shouldn’t\n",
    "be updated during training, to avoid forgetting what they already know. The large gradient\n",
    "updates triggered by the randomly initialized layers would be disruptive to the\n",
    "already-learned features.\n",
    "\n",
    "### TRAINING AND EVALUATING THE MODEL\n",
    "Compile and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx20.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx21.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model quickly starts overfitting, which is unsurprising given the small number of\n",
    "training samples. Validation accuracy has high variance for the same reason, but it\n",
    "seems to reach the high 50s.\n",
    "\n",
    "Note that your mileage may vary: because you have so few training samples, performance\n",
    "is heavily dependent on exactly which 200 samples you choose—and you’re\n",
    "choosing them at random. If this works poorly for you, try choosing a different random\n",
    "set of 200 samples, for the sake of the exercise (in real life, you don’t get to\n",
    "choose your training data).\n",
    "\n",
    "You can also train the same model without loading the pretrained word embeddings\n",
    "and without freezing the embedding layer. In that case, you’ll learn a taskspecific\n",
    "embedding of the input tokens, which is generally more powerful than\n",
    "pretrained word embeddings when lots of data is available. But in this case, you have\n",
    "only 200 training samples. Let’s try it (see figures 6.7 and 6.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx22.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th><img src=\"photos/tx23.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy stalls in the low 50s. So in this case, pretrained word embeddings\n",
    "outperform jointly learned embeddings. If you increase the number of training samples,\n",
    "this will quickly stop being the case—try it as an exercise.\n",
    "Finally, let’s evaluate the model on the test data. First, you need to tokenize the test\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx24.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx25.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load and evaluate the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx26.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "Now you’re able to do the following:\n",
    "- Turn raw text into something a neural network can process\n",
    "- Use the Embedding layer in a Keras model to learn task-specific token embeddings\n",
    "- Use pretrained word embeddings to get an extra boost on small natural language processing problems\n",
    "\n",
    "# Sequence processing with convnets\n",
    "In chapter 5, you learned about convolutional neural networks (convnets) and how\n",
    "they perform particularly well on computer vision problems, due to their ability to\n",
    "operate convolutionally, extracting features from local input patches and allowing for\n",
    "representation modularity and data efficiency. The same properties that make convnets\n",
    "excel at computer vision also make them highly relevant to sequence processing.\n",
    "Time can be treated as a spatial dimension, like the height or width of a 2D image.\n",
    "Such 1D convnets can be competitive with RNNs on certain sequence-processing\n",
    "problems, usually at a considerably cheaper computational cost. Recently, 1D convnets,\n",
    "typically used with dilated kernels, have been used with great success for audio\n",
    "generation and machine translation. In addition to these specific successes, it has long\n",
    "been known that small 1D convnets can offer a fast alternative to RNNs for simple tasks\n",
    "such as text classification and timeseries forecasting.\n",
    "\n",
    "## Understanding 1D convolution for sequence data\n",
    "The convolution layers introduced previously were 2D convolutions, extracting 2D\n",
    "patches from image tensors and applying an identical transformation to every patch.\n",
    "In the same way, you can use 1D convolutions, extracting local 1D patches (subsequences)\n",
    "from sequences (see figure 6.26)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx27.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such 1D convolution layers can recognize local patterns in a sequence. Because the\n",
    "same input transformation is performed on every patch, a pattern learned at a certain\n",
    "position in a sentence can later be recognized at a different position, making 1D convnets\n",
    "translation invariant (for temporal translations). For instance, a 1D convnet processing\n",
    "sequences of characters using convolution windows of size 5 should be able to\n",
    "learn words or word fragments of length 5 or less, and it should be able to recognize these words in any context in an input sequence. A character-level 1D convnet is thus\n",
    "able to learn about word morphology.\n",
    "\n",
    "### 1D pooling for sequence data\n",
    "You’re already familiar with 2D pooling operations, such as 2D average pooling and\n",
    "max pooling, used in convnets to spatially downsample image tensors. The 2D pooling\n",
    "operation has a 1D equivalent: extracting 1D patches (subsequences) from an input\n",
    "and outputting the maximum value (max pooling) or average value (average pooling).\n",
    "Just as with 2D convnets, this is used for reducing the length of 1D inputs (subsampling).\n",
    "\n",
    "### Implementing a 1D convnet\n",
    "In Keras, you use a 1D convnet via the Conv1D layer, which has an interface similar to\n",
    "Conv2D. It takes as input 3D tensors with shape (samples, time, features) and\n",
    "returns similarly shaped 3D tensors. The convolution window is a 1D window on the\n",
    "temporal axis: axis 1 in the input tensor.\n",
    "Let’s build a simple two-layer 1D convnet and apply it to the IMDB sentiment classification\n",
    "task you’re already familiar with. As a reminder, this is the code for\n",
    "obtaining and preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx28.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D convnets are structured in the same way as their 2D counterparts, which you used\n",
    "in chapter 5: they consist of a stack of Conv1D and MaxPooling1D layers, ending in\n",
    "either a global pooling layer or a Flatten layer, that turn the 3D outputs into 2D outputs,\n",
    "allowing you to add one or more Dense layers to the model for classification or\n",
    "regression.\n",
    "\n",
    "One difference, though, is the fact that you can afford to use larger convolution\n",
    "windows with 1D convnets. With a 2D convolution layer, a 3 × 3 convolution window\n",
    "contains 3 × 3 = 9 feature vectors; but with a 1D convolution layer, a convolution window\n",
    "of size 3 contains only 3 feature vectors. You can thus easily afford 1D convolution\n",
    "windows of size 7 or 9.\n",
    "\n",
    "This is the example 1D convnet for the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx29.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures 6.27 and 6.28 show the training and validation results. Validation accuracy is\n",
    "somewhat less than that of the LSTM, but runtime is faster on both CPU and GPU (the\n",
    "exact increase in speed will vary greatly depending on your exact configuration). At this\n",
    "point, you could retrain this model for the right number of epochs (eight) and run it\n",
    "on the test set. This is a convincing demonstration that a 1D convnet can offer a fast,\n",
    "cheap alternative to a recurrent network on a word-level sentiment-classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx30.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx31.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining CNNs and RNNs to process long sequences\n",
    "Because 1D convnets process input patches independently, they aren’t sensitive to the\n",
    "order of the timesteps (beyond a local scale, the size of the convolution windows),\n",
    "unlike RNNs. Of course, to recognize longer-term patterns, you can stack many convolution\n",
    "layers and pooling layers, resulting in upper layers that will see long chunks of\n",
    "the original inputs—but that’s still a fairly weak way to induce order sensitivity. One\n",
    "way to evidence this weakness is to try 1D convnets on the temperature-forecasting\n",
    "problem, where order-sensitivity is key to producing good predictions. The following\n",
    "example reuses the following variables defined previously: float_data, train_gen,\n",
    "val_gen, and val_steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx32.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th><img src=\"photos/tx33.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation MAE stays in the 0.40s: you can’t even beat the common-sense baseline\n",
    "using the small convnet. Again, this is because the convnet looks for patterns anywhere\n",
    "in the input timeseries and has no knowledge of the temporal position of a pattern\n",
    "it sees (toward the beginning, toward the end, and so on). Because more recent\n",
    "data points should be interpreted differently from older data points in the case of this\n",
    "specific forecasting problem, the convnet fails at producing meaningful results. This\n",
    "limitation of convnets isn’t an issue with the IMDB data, because patterns of keywords\n",
    "associated with a positive or negative sentiment are informative independently of\n",
    "where they’re found in the input sentences.\n",
    "\n",
    "One strategy to combine the speed and lightness of convnets with the order-sensitivity\n",
    "of RNNs is to use a 1D convnet as a preprocessing step before an RNN (see figure 6.30).\n",
    "This is especially beneficial when you’re dealing\n",
    "with sequences that are so long they can’t\n",
    "realistically be processed with RNNs, such as\n",
    "sequences with thousands of steps. The convnet\n",
    "will turn the long input sequence into\n",
    "much shorter (downsampled) sequences of\n",
    "higher-level features. This sequence of\n",
    "extracted features then becomes the input to\n",
    "the RNN part of the network.\n",
    "\n",
    "This technique isn’t seen often in\n",
    "research papers and practical applications,\n",
    "possibly because it isn’t well known. It’s effective\n",
    "and ought to be more common. Let’s try\n",
    "it on the temperature-forecasting dataset.\n",
    "Because this strategy allows you to manipulate\n",
    "much longer sequences, you can either look at data from longer ago (by increasing the lookback parameter of the data generator)\n",
    "or look at high-resolution timeseries (by decreasing the step parameter of the\n",
    "generator). Here, somewhat arbitrarily, you’ll use a step that’s half as large, resulting\n",
    "in a timeseries twice as long, where the temperature data is sampled at a rate of\n",
    "1 point per 30 minutes. The example reuses the generator function defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx34.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx35.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model, starting with two Conv1D layers and following up with a GRU layer.\n",
    "Figure 6.31 shows the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx36.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tx37.png\" alt=\"Drawing\" style=\"width:1000px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the validation loss, this setup isn’t as good as the regularized GRU alone,\n",
    "but it’s significantly faster. It looks at twice as much data, which in this case doesn’t\n",
    "appear to be hugely helpful but may be important for other datasets.\n",
    "## Wrapping up\n",
    "Here’s what you should take away from this section:\n",
    "- In the same way that 2D convnets perform well for processing visual patterns in 2D space, 1D convnets perform well for processing temporal patterns. They offer a faster alternative to RNNs on some problems, in particular natural language processing tasks.\n",
    "- Typically, 1D convnets are structured much like their 2D equivalents from the world of computer vision: they consist of stacks of Conv1D layers and Max- Pooling1D layers, ending in a global pooling operation or flattening operation.\n",
    "- Because RNNs are extremely expensive for processing very long sequences, but 1D convnets are cheap, it can be a good idea to use a 1D convnet as a preprocessing step before an RNN, shortening the sequence and extracting useful representations for the RNN to process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
