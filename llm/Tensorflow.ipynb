---
noteId: "8d921330291711ecbba71f7fb5ea1bc2"
tags: []

---

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystify the TensorFlow APIs\n",
    "\n",
    "TensorFlow, open-sourced in late 2015, is the most popular deep learning framework. It is for both researchers and developers.\n",
    "\n",
    "TensorFlow is flexible:\n",
    "\n",
    "   - write TensorFlow code in high level APIs such as tf.Keras (recommended), Estimators, or in low level APIs if you need greater control.\n",
    "   - write TensorFlow code in different languages: Python, JavaScript or SWIFT.\n",
    "   - train on CPU, GPU or TPU and deploy to mobile (Android / iOS), Web, Android Things, Raspberry Pi, and AIY kits etc.\n",
    "\n",
    "The flexibility of TensorFlow also brings complexity to its architecture. I often hear questions (from both beginners and expert TensorFlow users) like these “What’s the difference between tf.Keras and the other TensorFlow high level APIs? Which one do I choose?” So to help clarify some of the commonly asked questions:\n",
    "\n",
    "   - tf.Keras or Keras?\n",
    "   - tf.Keras or Estimators?\n",
    "   - Low level TensorFlow APIs or high level APIs?\n",
    "   - How does eager execution fit into all of this?\n",
    "\n",
    "## TensorFlow (Model Building) APIs\n",
    "\n",
    "First let’s start with the various options of the TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tf1.png\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the tf.layers API is going away with TensorFlow 2.0. It is recommended that you use tf.Keras instead of tf.layers API going forward.\n",
    "\n",
    "## tf.keras or Keras?\n",
    "\n",
    "If you are new to deep learning and just getting started, start with tf.Keras which is Keras integrated as part of the TensorFlow core API. Keras continues as an independent open source project.\n",
    "\n",
    "   - No more questions of “do I use Keras or TensorFlow?” because Keras is now part of TensorFlow.\n",
    "   - No need for “Keras with TensorFlow as the backend” because Keras is now part of TensorFlow. You can continue to use Theano and CNTK etc. as Keras backend if you wish.\n",
    "\n",
    "So instead of importing keras as a separate package:\n",
    "\n",
    "    pip install keras\n",
    "    import keras as keras\n",
    "\n",
    "Use keras as part of the TensorFlow core API:\n",
    "\n",
    "    pip install --upgrade tensorflow\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "**FAQ: “Why should I use tf.Keras?”** Use tf.Keras instead of Keras for better integration with other TensorFlow APIs such as eager execution and tf.data etc.\n",
    "\n",
    "There are three ways of creating a model in tf.Keras:\n",
    "\n",
    "   - **Sequential API —** with the Sequential API you are able to define and train a image classification model using ~10 lines of code.\n",
    "   - **Functional API —** Check out the tutorial on how to classify Fashion-MNIST with tf.Keras as an example.\n",
    "   - **Model subclassing —** see the TensorFlow Keras Guide on Tensorflow.org for examples on this. There are also lots other great tf.keras tutorials and sample code there.\n",
    "\n",
    "**Protip:** Deep Learning with Python by François Chollet is a great book for learning Keras.\n",
    "\n",
    "## tf.Keras or Estimators?\n",
    "\n",
    "Estimators is another high level TensorFlow API that simplifies the ML process by encapsulating the steps of training, evaluation, prediction, and export for serving.\n",
    "\n",
    "There are two types of estimators: 1) premade Estimators and 2) custom Estimators. According to the official TensorFlow documentation, the difference between the premade and custom Estimator is on the model function:\n",
    "\n",
    "   - Premade Estimator: the model function is already written for you. These are models in a box for you when you don’t want to train your own model.\n",
    "   - Custom Estimator: you need to write your own model function.\n",
    "\n",
    "It is recommended that you explore a premade Estimator to see whether that solves your problem, before writing your own custom Estimator.\n",
    "### Premade Estimators\n",
    "\n",
    "Premade Estimators are subclasses of the tf.estimator.Estimator base class. Examples of premade Estimators include DNNClassifier, LinearClassifier, LinearRegressor etc.\n",
    "### Custom Estimators\n",
    "\n",
    "As of today, there are 3 possibilities for defining the model for creating a custom Estimator:\n",
    "\n",
    "   - Define your model with Keras and then turn it into an estimator.\n",
    "   - Use a module from the TensorFlow Hub, a library for reusable machine learning modules. [Feedback for the TensorFlow team: please provide us with some good examples of how TensorFlow Hub works with tf.Keras.]\n",
    "    Write the function with the layers’s API. See example here. Note: tf.layers is going away with TensorFlow 2.0 but you can use tf.keras.layers instead for this purpose.\n",
    "\n",
    "**FAQ — “Do I need to use the Estimators API?”** In my opinion, go with tf.Keras unless you want to make use of the premade estimators. Note: with the recent TensorFlow 1.13 release, Keras models can now be directly exported to the SavedModel format(tf.contrib.saved_model.save_keras_model()) and used with Tensorflow Serving.\n",
    "\n",
    "## TensorFlow low level APIs\n",
    "\n",
    "With the low level APIs you will be working with Tensors, Constant, Placeholders, Graphs, and session etc. directly. A few learning resources:\n",
    "\n",
    "   - TensorFlow official documentation on low level TensorFlow\n",
    "   - Blogpost TensorFlow: The Confusing Parts, written by Jacob Buckman with details on this.\n",
    "\n",
    "**FAQ: “When do I need to write low level TensorFlow code?”** You only need to work at this low level if you are a student studying deep learning or researcher that needs greater control for your experiments; otherwise stick with the high level APIs.\n",
    "\n",
    "## Eager Execution\n",
    "\n",
    "Eager execution was announced in late 2017 in the blog post Eager Execution: An imperative, define-by-run interface to TensorFlow. It moved out of contrib since TensorFlow 1.7.\n",
    "\n",
    "This is an important feature for TensorFlow that enables you to debug TensorFlow without creating a graph — calling session.run(). Eager execution makes TensorFlow much more intuitive and pythonic. You just need one line of code to enable Eager Execution:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.contrib.eager as tfe\n",
    "\n",
    "    # Enable eager execution\n",
    "    tfe.enable_eager_execution()\n",
    "\n",
    "Here is a great example of how debugging is made easier with eager mode enabled.\n",
    "\n",
    "**Protip:** use defun to boost performance when in eager mode.\n",
    "\n",
    "Eager execution can be slower than executing the equivalent graph as it can’t benefit from whole-program optimizations on the graph, and also incurs overheads of interpreting Python code. You can use tf.contrib.eager.defun to create graph functions and get a performance boost. For example call defun on your training step like this:\n",
    "\n",
    "    # define train_step function\n",
    "    ...\n",
    "\n",
    "    # use defun on train_step\n",
    "\n",
    "    train_step = tf.contrib.eager.defun(train_step)\n",
    "\n",
    "**Protip:** Use Eager execution for research, and for production you should still use Graph execution.\n",
    "\n",
    "There is a new TensorFlow feature called Autograph that will make the conversion between the eager and graph mode seamlessly — read the blog post by the TensorFlow team. Also with TensorFlow 2.0, eager mode will become the default.\n",
    "\n",
    "**In Summary**, enable eager execution and write your TensorFlow code in tf.Keras. The TensorFlow team is working hard to make the process for us as simple as possible. They also provided an official Guidance on High-level APIs in TensorFlow 2.0 with more details on the topics I discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graphs\n",
    "\n",
    "[Graphs and Sessions](https://www.tensorflow.org/guide/graphs)\n",
    "\n",
    "[Tensorflow: The Confusing Parts](https://jacobbuckman.com/2018-06-25-tensorflow-the-confusing-parts-1/)\n",
    "\n",
    "[Tensorflow: The Confusing Parts](https://jacobbuckman.com/2018-09-17-tensorflow-the-confusing-parts-2/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion-MNIST with tf.Keras\n",
    "\n",
    "This is a tutorial of how to classify the Fashion-MNIST dataset with tf.keras, using a Convolutional Neural Network (CNN) architecture. In just a few lines of code, you can define and train a model that is able to classify the images with over 90% accuracy, even without much optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tf2.png\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST can be used as drop-in replacement for the original MNIST dataset (10 categories of handwritten digits). It shares the same image size (28x28) and structure of training (60,000) and testing (10,000) splits. It’s great for writing “hello world” tutorials for deep learning.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are ten categories to classify in the fashion_mnist dataset:\n",
    "\n",
    "Label Description \n",
    "0 T-shirt/top\n",
    "1 Trouser \n",
    "2 Pullover \n",
    "3 Dress \n",
    "4 Coat \n",
    "5 Sandal \n",
    "6 Shirt \n",
    "7 Sneaker \n",
    "8 Bag \n",
    "9 Ankle boot\n",
    "### Import the fashion_mnist dataset\n",
    "\n",
    "Let’s import the dataset and prepare it for training, validation and test.\n",
    "\n",
    "Load the fashion_mnist data with the keras.datasets API with just one line of code. Then another line of code to load the train and test dataset. Each gray scale image is 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the fashion-mnist pre-shuffled train data and test data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "What I like the best about Jupyter Notebook is the visualization. And you can visualize an image from the training data set with matplotlib library’s imshow() to take a look at one of the images from the datasets. Note each image is gray scale in the shape of 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a3b593828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show one of the images from the training dataset\n",
    "plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "We then normalize the data dimensions so that they are of approximately the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into train and validate arrays (will be used later)\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train/validation/test datasets\n",
    "\n",
    "In the earlier step of importing the date, we had 60,000 datasets for training and 10,000 test datasets. Now we further split the training data into train/validation. Here is how each type of dateset is used in deep learning:\n",
    "\n",
    "   - **Training data —** used for training the model\n",
    "   - **Validation data —** used for tuning the hyperparameters and evaluate the models\n",
    "   - **Test data —** used to test the model after the model has gone through initial vetting by the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (48000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "x_validate shape: (12000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "im_rows = 28\n",
    "im_cols = 28\n",
    "batch_size = 64\n",
    "im_shape = (im_rows, im_cols, 1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], *im_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *im_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *im_shape)\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('x_validate shape: {}'.format(x_validate.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Let’s define the model and train it.\n",
    "## Create the model architecture\n",
    "\n",
    "There are two APIs for defining a model in Keras:\n",
    "\n",
    "   1. Sequential model API\n",
    "   2. Functional API\n",
    "\n",
    "In this tutorial we are using the Sequential model API to create a simple CNN model repeating a few layers of a convolution layer followed by a pooling layer then a dropout layer. \n",
    "\n",
    "Note you only need to define the input data shape with the first layer. The last layers is a dense layer with softmax activation that classifies the 10 categories of data in fashion_mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 241,546\n",
      "Trainable params: 241,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = tf.keras.Sequential()\n",
    "# Add convolution 2D\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(im_rows, im_cols, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "\n",
    "We use model.compile() to configure the learning process before training the model. This is where you define the type of loss function, optimizer and the metrics evaluated by the model during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "We will train the model with a batch_size of 64 and 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 26s 533us/step - loss: 0.5021 - acc: 0.8156 - val_loss: 0.3394 - val_acc: 0.8776\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 25s 530us/step - loss: 0.3220 - acc: 0.8823 - val_loss: 0.2740 - val_acc: 0.9012\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 26s 548us/step - loss: 0.2726 - acc: 0.9002 - val_loss: 0.2676 - val_acc: 0.9032\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 27s 566us/step - loss: 0.2372 - acc: 0.9122 - val_loss: 0.2673 - val_acc: 0.9011\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 27s 567us/step - loss: 0.2089 - acc: 0.9230 - val_loss: 0.2455 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb31b01128>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, verbose=1,\n",
    "         validation_data=(x_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2700\n",
      "Test acc: 0.9010\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('Test loss: {:.4f}'.format(score[0]))\n",
    "print('Test acc: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
