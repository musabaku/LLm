{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Neural Networks and Deep Learning\n",
    " \n",
    " The chapter will teach you about:\n",
    "\n",
    "   * Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data\n",
    "   * Deep learning, a powerful set of techniques for learning in neural networks \n",
    "\n",
    "Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning. \n",
    "\n",
    "## Using neural nets to recognize handwritten digits\n",
    "The human visual system is one of the wonders of the world. Consider the following sequence of handwritten digits: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/digits.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Most people effortlessly recognize those digits as 504192. That ease is deceptive. In each hemisphere of our brain, humans have a primary visual cortex, also known as V1, containing 140 million neurons, with tens of billions of connections between them. And yet human vision involves not just V1, but an entire series of visual cortices - V2, V3, V4, and V5 - doing progressively more complex image processing. We carry in our heads a supercomputer, tuned by evolution over hundreds of millions of years, and superbly adapted to understand the visual world. Recognizing handwritten digits isn't easy. Rather, we humans are stupendously, astoundingly good at making sense of what our eyes show us. But nearly all that work is done unconsciously. And so we don't usually appreciate how tough a problem our visual systems solve.\n",
    "\n",
    "The difficulty of visual pattern recognition becomes apparent if you attempt to write a computer program to recognize digits like those above. What seems easy when we do it ourselves suddenly becomes extremely difficult. Simple intuitions about how we recognize shapes - \"a 9 has a loop at the top, and a vertical stroke in the bottom right\" - turn out to be not so simple to express algorithmically. When you try to make such rules precise, you quickly get lost in a morass of exceptions and caveats and special cases. It seems hopeless.\n",
    "\n",
    "Neural networks approach the problem in a different way. The idea is to take a large number of handwritten digits, known as training examples,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/mnist_100_digits.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then develop a system which can learn from those training examples. In other words, the neural network uses the examples to automatically infer rules for recognizing handwritten digits. Furthermore, by increasing the number of training examples, the network can learn more about handwriting, and so improve its accuracy. So while I've shown just 100 training digits above, perhaps we could build a better handwriting recognizer by using thousands or even millions or billions of training examples.\n",
    "\n",
    "In this chapter we'll write a computer program implementing a neural network that learns to recognize handwritten digits. The program is just 74 lines long, and uses no special neural network libraries. But this short program can recognize digits with an accuracy over 96 percent, without human intervention. Furthermore, in later chapters we'll develop ideas which can improve accuracy to over 99 percent. In fact, the best commercial neural networks are now so good that they are used by banks to process cheques, and by post offices to recognize addresses.\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "What is a neural network? To get started, I'll explain a type of artificial neuron called a perceptron. Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. Today, it's more common to use other models of artificial neurons - in this book, and in much modern work on neural networks, the main neuron model used is one called the sigmoid neuron. We'll get to sigmoid neurons shortly. But to understand why sigmoid neurons are defined the way they are, it's worth taking the time to first understand perceptrons.\n",
    "\n",
    "So how do perceptrons work? A perceptron takes several binary inputs, $x_1,x_2,…\n",
    ",$ and produces a single binary output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz0.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the example shown the perceptron has three inputs, $x_1,x_2,x_3$. In general it could have more or fewer inputs. Rosenblatt proposed a simple rule to compute the output. He introduced weights, $w_1,w_2,…,$ real numbers expressing the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined by whether the weighted $\\sum_j w_j x_j$ is less than or greater than some **threshold value**. Just like the weights, the threshold is a real number which is a parameter of the neuron. To put it in more precise algebraic terms: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray}\n",
    "\n",
    " That's all there is to how a perceptron works!\n",
    "\n",
    "That's the basic mathematical model. A way you can think about the perceptron is that it's a device that makes decisions by weighing up evidence. Let me give an example. It's not a very realistic example, but it's easy to understand, and we'll soon get to more realistic examples. Suppose the weekend is coming up, and you've heard that there's going to be a cheese festival in your city. You like cheese, and are trying to decide whether or not to go to the festival. You might make your decision by weighing up three factors: \n",
    "\n",
    "1. Is the weather good?\n",
    "2. Does your boyfriend or girlfriend want to accompany you?\n",
    "3. Is the festival near public transit? (You don't own a car). \n",
    "\n",
    "We can represent these three factors by corresponding binary variables $x_1,x_2,$ and $x_3$. For instance, we'd have $x_1=1$ if the weather is good, and $x_1=0$ if the weather is bad. Similarly, $x_2=1$ if your boyfriend or girlfriend wants to go, and $x_2=0$ if not. And similarly again for $x_3$ and public transit.\n",
    "\n",
    "Now, suppose you absolutely adore cheese, so much so that you're happy to go to the festival even if your boyfriend or girlfriend is uninterested and the festival is hard to get to. But perhaps you really loathe bad weather, and there's no way you'd go to the festival if the weather is bad. You can use perceptrons to model this kind of decision-making. One way to do this is to choose a weight $w_1=6$ for the weather, and $w_2=2$ and $w_3=2$ for the other conditions. The larger value of $w_1$ indicates that the weather matters a lot to you, much more than whether your boyfriend or girlfriend joins you, or the nearness of public transit. Finally, suppose you choose a threshold of 5 for the perceptron. With these choices, the perceptron implements the desired decision-making model, outputting 1 whenever the weather is good, and 0 whenever the weather is bad. It makes no difference to the output whether your boyfriend or girlfriend wants to go, or whether public transit is nearby.\n",
    "\n",
    "By varying the weights and the threshold, we can get different models of decision-making. For example, suppose we instead chose a threshold of 3. Then the perceptron would decide that you should go to the festival whenever the weather was good or when both the festival was near public transit and your boyfriend or girlfriend was willing to join you. In other words, it'd be a different model of decision-making. Dropping the threshold means you're more willing to go to the festival.\n",
    "\n",
    "Obviously, the perceptron isn't a complete model of human decision-making! But what the example illustrates is how a perceptron can weigh up different kinds of evidence in order to make decisions. And it should seem plausible that a complex network of perceptrons could make quite subtle decisions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz1.png\" alt=\"Drawing\" style=\"width:600px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this network, the first column of perceptrons - what we'll call the first layer of perceptrons - is making three very simple decisions, by weighing the input evidence. What about the perceptrons in the second layer? Each of those perceptrons is making a decision by weighing up the results from the first layer of decision-making. In this way a perceptron in the second layer can make a decision at a more complex and more abstract level than perceptrons in the first layer. And even more complex decisions can be made by the perceptron in the third layer. In this way, a many-layer network of perceptrons can engage in sophisticated decision making.\n",
    "\n",
    "Incidentally, when I defined perceptrons I said that a perceptron has just a single output. In the network above the perceptrons look like they have multiple outputs. In fact, they're still single output. The multiple output arrows are merely a useful way of indicating that the output from a perceptron is being used as the input to several other perceptrons. It's less unwieldy than drawing a single output line which then splits.\n",
    "\n",
    "Let's simplify the way we describe perceptrons. The condition $\\sum_j\n",
    "w_j x_j > \\mbox{threshold}$ is cumbersome, and we can make two notational changes to simplify it. The first change is to write $\\sum_j w_j x_j$ as a dot product, $w \\cdot x \\equiv \\sum_j w_j x_j$, where w and x are vectors whose components are the weights and inputs, respectively. The second change is to move the threshold to the other side of the inequality, and to replace it by what's known as the perceptron's bias, b≡−threshold. Using the bias instead of the threshold, the perceptron rule can be rewritten:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\tag{2}\\end{eqnarray}\n",
    "\n",
    "You can think of the bias as a measure of how easy it is to get the perceptron to output a 1. Or to put it in more biological terms, **the bias is a measure of how easy it is to get the perceptron to fire**. For a perceptron with a really big bias, it's extremely easy for the perceptron to output a 1. But if the bias is very negative, then it's difficult for the perceptron to output a 1. Obviously, introducing the bias is only a small change in how we describe perceptrons, but we'll see later that it leads to further notational simplifications. Because of this, in the remainder of the book we won't use the threshold, we'll always use the bias.\n",
    "\n",
    "I've described perceptrons as a method for weighing evidence to make decisions. Another way perceptrons can be used is to compute the elementary logical functions we usually think of as underlying computation, functions such as **AND, OR, and NAND**. For example, suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3. Here's our perceptron: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz2.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then we see that input 00 produces output 1, since $(-2)*0+(-2)*0+3 = 3$ is positive. Here, I've introduced the ∗ symbol to make the multiplications explicit. Similar calculations show that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since $(−2)∗1+(−2)∗1+3=−1$ is negative. And so our perceptron implements a **NAND** gate!\n",
    " \n",
    " The NAND example shows that we can use perceptrons to compute simple logical functions. In fact, we can use networks of perceptrons to compute any logical function at all. The reason is that the NAND gate is universal for computation, that is, we can build any computation up out of NAND gates.\n",
    " \n",
    "It turns out that we can devise learning algorithms which can automatically tune the weights and biases of a network of artificial neurons. This tuning happens in response to external stimuli, without direct intervention by a programmer. These learning algorithms enable us to use artificial neurons in a way which is radically different to conventional logic gates. Instead of explicitly laying out a circuit of NAND and other gates, our neural networks can simply learn to solve problems, sometimes problems where it would be extremely difficult to directly design a conventional circuit.\n",
    "\n",
    "### Sigmoid neurons\n",
    "\n",
    "Learning algorithms sound terrific. But how can we devise such algorithms for a neural network? Suppose we have a network of perceptrons that we'd like to use to learn to solve some problem. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit. And we'd like the network to learn weights and biases so that the output from the network correctly classifies the digit. To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network. As we'll see in a moment, this property will make learning possible. Schematically, here's what we want (obviously this network is too simple to do handwriting recognition!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz8.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it were true that a small change in a weight (or bias) causes only a small change in output, then we could use this fact to modify the weights and biases to get our network to behave more in the manner we want. For example, suppose the network was mistakenly classifying an image as an \"8\" when it should be a \"9\". We could figure out how to make a small change in the weights and biases so the network gets a little closer to classifying the image as a \"9\". And then we'd repeat this, changing the weights and biases over and over to produce better and better output. The network would be learning.\n",
    "\n",
    "The problem is that this isn't what happens when our network contains perceptrons. In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 0\n",
    "to 1. That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way. So while your \"9\" might now be classified correctly, the behaviour of the network on all the other images is likely to have completely changed in some hard-to-control way. That makes it difficult to see how to gradually modify the weights and biases so that the network gets closer to the desired behaviour. Perhaps there's some clever way of getting around this problem. But it's not immediately obvious how we can get a network of perceptrons to learn.\n",
    "\n",
    "We can overcome this problem by introducing a new type of artificial neuron called a **sigmoid neuron**. Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output. That's the crucial fact which will allow a network of sigmoid neurons to learn.\n",
    "\n",
    "Okay, let me describe the sigmoid neuron. We'll depict sigmoid neurons in the same way we depicted perceptrons: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz9.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a perceptron, the sigmoid neuron has inputs, $x_1,x_2,….$ But instead of being just 0 or 1, these inputs can also take on any values between 0 and 1. So, for instance, 0.638… is a valid input for a sigmoid neuron. Also just like a perceptron, the sigmoid neuron has weights for each input, $w_1,w_2,…,$ and an overall bias, b. But the output is not 0 or 1. Instead, it's $\\sigma(w \\cdot x+b)$, where σ is called the sigmoid function* *Incidentally, σ is sometimes called the logistic function, and this new class of neurons called logistic neurons. It's useful to remember this terminology, since these terms are used by many people working with neural nets. However, we'll stick with the sigmoid terminology., and is defined by: \n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\tag{3}\\end{eqnarray}\n",
    "\n",
    " To put it all a little more explicitly, the output of a sigmoid neuron with inputs $x_1,x_2,…,$ weights $w_1,w_2,…,$ and bias b is \n",
    " \n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
    "\\tag{4}\\end{eqnarray}\n",
    "\n",
    "At first sight, sigmoid neurons appear very different to perceptrons. The algebraic form of the sigmoid function may seem opaque and forbidding if you're not already familiar with it. In fact, there are many similarities between perceptrons and sigmoid neurons, and the algebraic form of the sigmoid function turns out to be more of a technical detail than a true barrier to understanding.\n",
    "\n",
    "To understand the similarity to the perceptron model, suppose $z\n",
    "\\equiv w \\cdot x + b$ is a large positive number. Then $e^{-z}\n",
    "\\approx 0$ and so σ(z)≈1. In other words, when z=w⋅x+b is large and positive, the output from the sigmoid neuron is approximately 1, just as it would have been for a perceptron. Suppose on the other hand that z=w⋅x+b is very negative. Then $e^{-z} \\rightarrow \\infty$, and σ(z)≈0. So when z=w⋅x+b is very negative, the behaviour of a sigmoid neuron also closely approximates a perceptron. It's only when w⋅x+b is of modest size that there's much deviation from the perceptron model.\n",
    "\n",
    "What about the algebraic form of σ ? How can we understand that? In fact, the exact form of σ isn't so important - what really matters is the shape of the function when plotted. Here's the shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/Selection_216.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape is a smoothed out version of a step function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/Selection_217.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If σ had in fact been a step function, then the sigmoid neuron would be a perceptron, since the output would be 1 or 0 depending on whether w⋅x+b was positive or negative* *Actually, when w⋅x+b=0 the perceptron outputs 0, while the step function outputs 1. So, strictly speaking, we'd need to modify the step function at that one point. But you get the idea.. By using the actual σ function we get, as already implied above, a smoothed out perceptron. Indeed, it's the smoothness of the σ function that is the crucial fact, not its detailed form. The smoothness of σ means that small changes $Δw_j$ in the weights and Δb in the bias will produce a small change Δoutput in the output from the neuron. In fact, calculus tells us that Δoutput is well approximated by \n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\tag{5}\\end{eqnarray}\n",
    "\n",
    " where the sum is over all the weights, $w_j$, and $∂output/∂w_j$ and ∂output/∂b denote partial derivatives of the output with respect to $w_j$ and b, respectively. Don't panic if you're not comfortable with partial derivatives! While the expression above looks complicated, with all the partial derivatives, it's actually saying something very simple (and which is very good news): Δoutput is a linear function of the changes $Δw_j$ and Δb in the weights and bias. This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change in the output. So while sigmoid neurons have much of the same qualitative behaviour as perceptrons, they make it much easier to figure out how changing the weights and biases will change the output.\n",
    "\n",
    "If it's the shape of σ\n",
    "which really matters, and not its exact form, then why use the particular form used for σ in Equation (3)? In fact, later in the book we will occasionally consider neurons where the output is f(w⋅x+b) for some other activation function f(⋅). The main thing that changes when we use a different activation function is that the particular values for the partial derivatives in Equation (5) change. It turns out that when we compute those partial derivatives later, using σ will simplify the algebra, simply because exponentials have lovely properties when differentiated. In any case, σ is commonly-used in work on neural nets, and is the activation function we'll use most often in this book.\n",
    "\n",
    "How should we interpret the output from a sigmoid neuron? Obviously, one big difference between perceptrons and sigmoid neurons is that sigmoid neurons don't just output 0 or 1. They can have as output any real number between 0 and 1, so values such as 0.173… and 0.689… are legitimate outputs. This can be useful, for example, if we want to use the output value to represent the average intensity of the pixels in an image input to a neural network. But sometimes it can be a nuisance. Suppose we want the output from the network to indicate either \"the input image is a 9\" or \"the input image is not a 9\". Obviously, it'd be easiest to do this if the output was a 0 or a 1, as in a perceptron. But in practice we can set up a convention to deal with this, for example, by deciding to interpret any output of at least 0.5 as indicating a \"9\", and any output less than 0.5 as indicating \"not a 9\". I'll always explicitly state when we're using such a convention, so it shouldn't cause any confusion.\n",
    "\n",
    "### The architecture of neural networks\n",
    "In the next section I'll introduce a neural network that can do a pretty good job classifying handwritten digits. In preparation for that, it helps to explain some terminology that lets us name different parts of a network. Suppose we have the network: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz10.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As mentioned earlier, the leftmost layer in this network is called the input layer, and the neurons within the layer are called input neurons. The rightmost or output layer contains the output neurons, or, as in this case, a single output neuron. The middle layer is called a hidden layer, since the neurons in this layer are neither inputs nor outputs. The term **\"hidden\"** perhaps sounds a little mysterious - the first time I heard the term I thought it must have some deep philosophical or mathematical significance - but it really means nothing more than \"not an input or an output\". The network above has just a single hidden layer, but some networks have multiple hidden layers. For example, the following four-layer network has two hidden layers: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz11.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons. I'm not going to use the MLP terminology in this book, since I think it's confusing, but wanted to warn you of its existence.\n",
    "\n",
    "The design of the input and output layers in a network is often straightforward. For example, suppose we're trying to determine whether a handwritten image depicts a \"9\" or not. A natural way to design the network is to encode the intensities of the image pixels into the input neurons. If the image is a 64\n",
    "by 64 greyscale image, then we'd have 4,096=64×64 input neurons, with the intensities scaled appropriately between 0 and 1. The output layer will contain just a single neuron, with output values of less than 0.5 indicating \"input image is not a 9\", and values greater than 0.5 indicating \"input image is a 9 \".\n",
    "\n",
    "While the design of the input and output layers of a neural network is often straightforward, there can be quite an art to the design of the hidden layers. In particular, it's not possible to sum up the design process for the hidden layers with a few simple rules of thumb. Instead, neural networks researchers have developed many design heuristics for the hidden layers, which help people get the behaviour they want out of their nets. For example, such heuristics can be used to help determine how to trade off the number of hidden layers against the time required to train the network. We'll meet several such design heuristics later in this book.\n",
    "\n",
    "Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks. This means there are no loops in the network - information is always fed forward, never fed back. If we did have loops, we'd end up with situations where the input to the σ function depended on the output. That'd be hard to make sense of, and so we don't allow such loops.\n",
    "\n",
    "However, there are other models of artificial neural networks in which feedback loops are possible. These models are called recurrent neural networks. The idea in these models is to have neurons which fire for some limited duration of time, before becoming quiescent. That firing can stimulate other neurons, which may fire a little while later, also for a limited duration. That causes still more neurons to fire, and so over time we get a cascade of neurons firing. Loops don't cause problems in such a model, since a neuron's output only affects its input at some later time, not instantaneously.\n",
    "\n",
    "Recurrent neural nets have been less influential than feedforward networks, in part because the learning algorithms for recurrent nets are (at least to date) less powerful. But recurrent networks are still extremely interesting. They're much closer in spirit to how our brains work than feedforward networks. And it's possible that recurrent networks can solve important problems which can only be solved with great difficulty by feedforward networks. However, to limit our scope, in this book we're going to concentrate on the more widely-used feedforward networks.\n",
    "\n",
    "### A simple network to classify handwritten digits\n",
    "Having defined neural networks, let's return to handwriting recognition. We can split the problem of recognizing handwritten digits into two sub-problems. First, we'd like a way of breaking an image containing many digits into a sequence of separate images, each containing a single digit. For example, we'd like to break the image into six separate images,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/digits_separate.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We humans solve this segmentation problem with ease, but it's challenging for a computer program to correctly break up the image. Once the image has been segmented, the program then needs to classify each individual digit. So, for instance, we'd like our program to recognize that the first digit above,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/mnist_first_digit.png\" alt=\"Drawing\" style=\"width:100px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a 5.\n",
    "\n",
    "We'll focus on writing a program to solve the second problem, that is, classifying individual digits. We do this because it turns out that the segmentation problem is not so difficult to solve, once you have a good way of classifying individual digits. There are many approaches to solving the segmentation problem. One approach is to trial many different ways of segmenting the image, using the individual digit classifier to score each trial segmentation. A trial segmentation gets a high score if the individual digit classifier is confident of its classification in all segments, and a low score if the classifier is having a lot of trouble in one or more segments. The idea is that if the classifier is having trouble somewhere, then it's probably having trouble because the segmentation has been chosen incorrectly. This idea and other variations can be used to solve the segmentation problem quite well. So instead of worrying about segmentation we'll concentrate on developing a neural network which can solve the more interesting and difficult problem, namely, recognizing individual handwritten digits.\n",
    "\n",
    "To recognize individual digits we will use a three-layer neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz12.png\" alt=\"Drawing\" style=\"width:500px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer of the network contains neurons encoding the values of the input pixels. As discussed in the next section, our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784=28×28 neurons. For simplicity I've omitted most of the 784 input neurons in the diagram above. The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey.\n",
    "\n",
    "The second layer of the network is a hidden layer. We denote the number of neurons in this hidden layer by n, and we'll experiment with different values for n. The example shown illustrates a small hidden layer, containing just n=15 neurons.\n",
    "\n",
    "The output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output ≈1, then that will indicate that the network thinks the digit is a 0. If the second neuron fires then that will indicate that the network thinks the digit is a 1. And so on. A little more precisely, we number the output neurons from 0 through 9, and figure out which neuron has the highest activation value. If that neuron is, say, neuron number 6, then our network will guess that the input digit was a 6. And so on for the other output neurons.\n",
    "\n",
    "You might wonder why we use 10 output neurons. After all, the goal of the network is to tell us which digit (0,1,2,…,9) corresponds to the input image. A seemingly natural way of doing that is to use just 4 output neurons, treating each neuron as taking on a binary value, depending on whether the neuron's output is closer to 0 or to 1. Four neurons are enough to encode the answer, since 24=16 is more than the 10 possible values for the input digit. Why should our network use 10 neurons instead? Isn't that inefficient? The ultimate justification is empirical: we can try out both network designs, and it turns out that, for this particular problem, the network with 10 output neurons learns to recognize digits better than the network with 4 output neurons. But that leaves us wondering why using 10 output neurons works better. Is there some heuristic that would tell us in advance that we should use the 10-output encoding instead of the 4 -output encoding?\n",
    "\n",
    "To understand why we do this, it helps to think about what the neural network is doing from first principles. Consider first the case where we use 10 output neurons. Let's concentrate on the first output neuron, the one that's trying to decide whether or not the digit is a 0. It does this by weighing up evidence from the hidden layer of neurons. What are those hidden neurons doing? Well, just suppose for the sake of argument that the first neuron in the hidden layer detects whether or not an image like the following is present:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/mnist_top_left_feature.png\" alt=\"Drawing\" style=\"width:100px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can do this by heavily weighting input pixels which overlap with the image, and only lightly weighting the other inputs. In a similar way, let's suppose for the sake of argument that the second, third, and fourth neurons in the hidden layer detect whether or not the following images are present:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/mnist_other_features.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, these four images together make up the 0 image that we saw in the line of digits shown earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/mnist_complete_zero.png\" alt=\"Drawing\" style=\"width:100px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if all four of these hidden neurons are firing then we can conclude that the digit is a 0. Of course, that's not the only sort of evidence we can use to conclude that the image was a 0 - we could legitimately get a 0 in many other ways (say, through translations of the above images, or slight distortions). But it seems safe to say that at least in this case we'd conclude that the input was a 0.\n",
    "\n",
    "Supposing the neural network functions in this way, we can give a plausible explanation for why it's better to have 10 outputs from the network, rather than 4. If we had 4 outputs, then the first output neuron would be trying to decide what the most significant bit of the digit was. And there's no easy way to relate that most significant bit to simple shapes like those shown above. It's hard to imagine that there's any good historical reason the component shapes of the digit will be closely related to (say) the most significant bit in the output.\n",
    "\n",
    "Now, with all that said, this is all just a heuristic. Nothing says that the three-layer neural network has to operate in the way I described, with the hidden neurons detecting simple component shapes. Maybe a clever learning algorithm will find some assignment of weights that lets us use only 4 output neurons. But as a heuristic the way of thinking I've described works pretty well, and can save you a lot of time in designing good neural network architectures.\n",
    "\n",
    "### Learning with gradient descent\n",
    "\n",
    "Now that we have a design for our neural network, how can it learn to recognize digits? The first thing we'll need is a data set to learn from - a so-called training data set. We'll use the MNIST data set, which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.\n",
    "\n",
    "The MNIST data comes in two parts. The first part contains 60,000 images to be used as training data. These images are scanned handwriting samples from 250 people, half of whom were US Census Bureau employees, and half of whom were high school students. The images are greyscale and 28 by 28 pixels in size. The second part of the MNIST data set is 10,000 images to be used as test data. Again, these are 28 by 28 greyscale images. We'll use the test data to evaluate how well our neural network has learned to recognize digits. To make this a good test of performance, the test data was taken from a different set of 250 people than the original training data (albeit still a group split between Census Bureau employees and high school students). This helps give us confidence that our system can recognize digits from people whose writing it didn't see during training.\n",
    "\n",
    "We'll use the notation x to denote a training input. It'll be convenient to regard each training input x as a 28×28=784-dimensional vector. Each entry in the vector represents the grey value for a single pixel in the image. We'll denote the corresponding desired output by y=y(x), where y is a 10-dimensional vector. For example, if a particular training image, x, depicts a 6, then y(x)=(0,0,0,0,0,0,1,0,0,0)T is the desired output from the network. Note that T here is the transpose operation, turning a row vector into an ordinary (column) vector.\n",
    "\n",
    "What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates y(x) for all training inputs x. To quantify how well we're achieving this goal we define a cost function* *Sometimes referred to as a loss or objective function. We use the term cost function throughout this book, but you should note the other terminology, since it's often used in research papers and other discussions of neural networks. : \n",
    "\n",
    "\\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\tag{6}\\end{eqnarray}\n",
    "\n",
    "Here, w denotes the collection of all weights in the network, b all the biases, n is the total number of training inputs, a is the vector of outputs from the network when x is input, and the sum is over all training inputs, x. Of course, the output a depends on x, w and b, but to keep the notation simple I haven't explicitly indicated this dependence. The notation ‖v‖ just denotes the usual length function for a vector v. We'll call C the **quadratic cost function**; it's also sometimes known as the **mean squared error or just MSE**. Inspecting the form of the quadratic cost function, we see that C(w,b) is non-negative, since every term in the sum is non-negative. Furthermore, the cost C(w,b) becomes small, i.e., C(w,b)≈0, precisely when y(x) is approximately equal to the output, a, for all training inputs, x. So our training algorithm has done a good job if it can find weights and biases so that C(w,b)≈0. By contrast, it's not doing so well when C(w,b) is large - that would mean that y(x) is not close to the output a for a large number of inputs. So the aim of our training algorithm will be to minimize the cost C(w,b) as a function of the weights and biases. In other words, we want to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as **gradient descent**.\n",
    "\n",
    " Why introduce the quadratic cost? After all, aren't we primarily interested in the number of images correctly classified by the network? Why not try to maximize that number directly, rather than minimizing a proxy measure like the quadratic cost? The problem with that is that the number of images correctly classified is not a smooth function of the weights and biases in the network. For the most part, making small changes to the weights and biases won't cause any change at all in the number of training images classified correctly. That makes it difficult to figure out how to change the weights and biases to get improved performance. If we instead use a smooth cost function like the quadratic cost it turns out to be easy to figure out how to make small changes in the weights and biases so as to get an improvement in the cost. That's why we focus first on minimizing the quadratic cost, and only after that will we examine the classification accuracy.\n",
    "\n",
    "Even given that we want to use a smooth cost function, you may still wonder why we choose the quadratic function used in Equation (6). Isn't this a rather ad hoc choice? Perhaps if we chose a different cost function we'd get a totally different set of minimizing weights and biases? This is a valid concern, and later we'll revisit the cost function, and make some modifications. However, the quadratic cost function of Equation (6) works perfectly well for understanding the basics of learning in neural networks, so we'll stick with it for now.\n",
    "\n",
    "Recapping, our goal in training a neural network is to find weights and biases which minimize the quadratic cost function C(w,b) . This is a well-posed problem, but it's got a lot of distracting structure as currently posed - the interpretation of w and b as weights and biases, the σ function lurking in the background, the choice of network architecture, MNIST, and so on. It turns out that we can understand a tremendous amount by ignoring most of that structure, and just concentrating on the minimization aspect. So for now we're going to forget all about the specific form of the cost function, the connection to neural networks, and so on. Instead, we're going to imagine that we've simply been given a function of many variables and we want to minimize that function. We're going to develop a technique called **gradient descent** which can be used to solve such minimization problems. Then we'll come back to the specific function we want to minimize for neural networks.\n",
    "\n",
    "Okay, let's suppose we're trying to minimize some function, C(v). This could be any real-valued function of many variables, v=v1,v2,…. Note that I've replaced the w and b notation by v to emphasize that this could be any function - we're not specifically thinking in the neural networks context any more. To minimize C(v) it helps to imagine C as a function of just two variables, which we'll call v1 and v2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/valley.png\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like is to find where C achieves its global minimum. Now, of course, for the function plotted above, we can eyeball the graph and find the minimum. In that sense, I've perhaps shown slightly too simple a function! A general function, C, may be a complicated function of many variables, and it won't usually be possible to just eyeball the graph to find the minimum.\n",
    "\n",
    "One way of attacking the problem is to use calculus to try to find the minimum analytically. We could compute derivatives and then try using them to find places where C\n",
    "is an extremum. With some luck that might work when C is a function of just one or a few variables. But it'll turn into a nightmare when we have many more variables. And for neural networks we'll often want far more variables - the biggest neural networks have cost functions which depend on billions of weights and biases in an extremely complicated way. Using calculus to minimize that just won't work!\n",
    "\n",
    "Okay, so calculus doesn't work. Fortunately, there is a beautiful analogy which suggests an algorithm which works pretty well. We start by thinking of our function as a kind of a valley. If you squint just a little at the plot above, that shouldn't be too hard. And we imagine a ball rolling down the slope of the valley. Our everyday experience tells us that the ball will eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to find a minimum for the function? We'd randomly choose a starting point for an (imaginary) ball, and then simulate the motion of the ball as it rolled down to the bottom of the valley. We could do this simulation simply by computing derivatives (and perhaps some second derivatives) of C those derivatives would tell us everything we need to know about the local \"shape\" of the valley, and therefore how our ball should roll.\n",
    "\n",
    "Based on what I've just written, you might suppose that we'll be trying to write down Newton's equations of motion for the ball, considering the effects of friction and gravity, and so on. Actually, we're not going to take the ball-rolling analogy quite that seriously - we're devising an algorithm to minimize C , not developing an accurate simulation of the laws of physics! The ball's-eye view is meant to stimulate our imagination, not constrain our thinking. So rather than get into all the messy details of physics, let's simply ask ourselves: if we were declared God for a day, and could make up our own laws of physics, dictating to the ball how it should roll, what law or laws of motion could we pick that would make it so the ball always rolled to the bottom of the valley?\n",
    "\n",
    "To make this question more precise, let's think about what happens when we move the ball a small amount $Δv_1$\n",
    "in the $v_1$ direction, and a small amount $Δv_2$ in the $v_2$ direction. Calculus tells us that C changes as follows:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\n",
    "  \\frac{\\partial C}{\\partial v_2} \\Delta v_2.\n",
    "\\tag{7}\\end{eqnarray}\n",
    "\n",
    "We're going to find a way of choosing $Δv_1$ and $Δv_2$ so as to make ΔC negative; i.e., we'll choose them so the ball is rolling down into the valley. To figure out how to make such a choice it helps to define Δv to be the vector of changes in v, $\\Delta v \\equiv (\\Delta v_1, \\Delta v_2)^T$, where T is again the transpose operation, turning row vectors into column vectors. We'll also define the gradient of C to be the vector of partial derivatives, $\\left(\\frac{\\partial\n",
    "    C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2}\\right)^T$. We denote the gradient vector by ∇C, i.e.: \n",
    "    \n",
    "\\begin{eqnarray} \n",
    "  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1}, \n",
    "  \\frac{\\partial C}{\\partial v_2} \\right)^T.\n",
    "\\tag{8}\\end{eqnarray}\n",
    "\n",
    "In a moment we'll rewrite the change ΔC in terms of Δv and the gradient, ∇C. Before getting to that, though, I want to clarify something that sometimes gets people hung up on the gradient. When meeting the ∇C notation for the first time, people sometimes wonder how they should think about the ∇ symbol. What, exactly, does ∇ mean? In fact, it's perfectly fine to think of ∇C as a single mathematical object - the vector defined above - which happens to be written using two symbols. In this point of view, ∇ is just a piece of notational flag-waving, telling you \"hey, ∇C is a gradient vector\". There are more advanced points of view where ∇ can be viewed as an independent mathematical entity in its own right (for example, as a differential operator), but we won't need such points of view.\n",
    "\n",
    "With these definitions, the expression (7) for ΔC can be rewritten as \n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\nabla C \\cdot \\Delta v.\n",
    "\\tag{9}\\end{eqnarray}\n",
    "\n",
    " This equation helps explain why ∇C is called the gradient vector: ∇C relates changes in v to changes in C, just as we'd expect something called a gradient to do. But what's really exciting about the equation is that it lets us see how to choose Δv so as to make ΔC negative. In particular, suppose we choose \n",
    " \n",
    " \\begin{eqnarray} \n",
    "  \\Delta v = -\\eta \\nabla C,\n",
    "\\tag{10}\\end{eqnarray}\n",
    "\n",
    " where η is a small, positive parameter (known as the learning rate). Then Equation (9) tells us that $\\Delta C \\approx -\\eta\n",
    "\\nabla C \\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2$. Because $\\Delta C \\leq 0$, this guarantees that ΔC≤0, i.e., C will always decrease, never increase, if we change v according to the prescription in (10). (Within, of course, the limits of the approximation in Equation (9)). This is exactly the property we wanted! And so we'll take Equation (10) to define the \"law of motion\" for the ball in our gradient descent algorithm. That is, we'll use Equation (10) to compute a value for Δv, then move the ball's position v by that amount: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "  v \\rightarrow v' = v -\\eta \\nabla C.\n",
    "\\tag{11}\\end{eqnarray}\n",
    "\n",
    " Then we'll use this update rule again, to make another move. If we keep doing this, over and over, we'll keep decreasing C until - we hope - we reach a global minimum.\n",
    "\n",
    "Summing up, the way the gradient descent algorithm works is to repeatedly compute the gradient ∇C\n",
    ", and then to move in the opposite direction, \"falling down\" the slope of the valley. We can visualize it like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/valley_with_ball.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with this rule gradient descent doesn't reproduce real physical motion. In real life a ball has momentum, and that momentum may allow it to roll across the slope, or even (momentarily) roll uphill. It's only after the effects of friction set in that the ball is guaranteed to roll down into the valley. By contrast, our rule for choosing Δv just says \"go down, right now\". That's still a pretty good rule for finding the minimum!\n",
    "\n",
    "To make gradient descent work correctly, we need to choose the learning rate η to be small enough that Equation (9) is a good approximation. If we don't, we might end up with ΔC>0, which obviously would not be good! At the same time, we don't want η to be too small, since that will make the changes Δv tiny, and thus the gradient descent algorithm will work very slowly. In practical implementations, η is often varied so that Equation (9) remains a good approximation, but the algorithm isn't too slow. We'll see later how this works.\n",
    "\n",
    "I've explained gradient descent when C is a function of just two variables. But, in fact, everything works just as well even when C is a function of many more variables. Suppose in particular that C is a function of m variables, $v_1,…,v_m$. Then the change ΔC in C produced by a small change $\\Delta v = (\\Delta v_1,\n",
    "\\ldots, \\Delta v_m)^T$ is \n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\nabla C \\cdot \\Delta v,\n",
    "\\tag{12}\\end{eqnarray}$$\n",
    "\n",
    " where the gradient ∇C is the vector \n",
    " \n",
    "$$ \\begin{eqnarray}\n",
    "  \\nabla C \\equiv \\left(\\frac{\\partial C}{\\partial v_1}, \\ldots, \n",
    "  \\frac{\\partial C}{\\partial v_m}\\right)^T.\n",
    "\\tag{13}\\end{eqnarray}$$\n",
    "\n",
    " Just as for the two variable case, we can choose \n",
    " \n",
    "$$ \\begin{eqnarray}\n",
    "  \\Delta v = -\\eta \\nabla C,\n",
    "\\tag{14}\\end{eqnarray}$$\n",
    "\n",
    " and we're guaranteed that our (approximate) expression (12) for ΔC will be negative. This gives us a way of following the gradient to a minimum, even when C is a function of many variables, by repeatedly applying the update rule \n",
    " \n",
    " $$\\begin{eqnarray}\n",
    "  v \\rightarrow v' = v-\\eta \\nabla C.\n",
    "\\tag{15}\\end{eqnarray}$$\n",
    "\n",
    "You can think of this update rule as defining the **gradient descent algorithm**. It gives us a way of repeatedly changing the position v in order to find a minimum of the function C. The rule doesn't always work - several things can go wrong and prevent gradient descent from finding the global minimum of C, a point we'll return to explore in later chapters. But, in practice gradient descent often works extremely well, and in neural networks we'll find that it's a powerful way of minimizing the cost function, and so helping the net learn.\n",
    "\n",
    "Indeed, there's even a sense in which gradient descent is the optimal strategy for searching for a minimum. Let's suppose that we're trying to make a move Δv in position so as to decrease C as much as possible. This is equivalent to minimizing $\\Delta C \\approx \\nabla C\n",
    "\\cdot \\Delta v$. We'll constrain the size of the move so that ‖Δv‖=ϵ for some small fixed ϵ>0. In other words, we want a move that is a small step of a fixed size, and we're trying to find the movement direction which decreases C as much as possible. It can be proved that the choice of Δv which minimizes $\\nabla C \\cdot \\Delta v$ is $\\Delta v = - \\eta \\nabla C$, where $\\eta = \\epsilon / \\|\\nabla C\\|$ is determined by the size constraint ‖Δv‖=ϵ. So gradient descent can be viewed as a way of taking small steps in the direction which does the most to immediately decrease C.\n",
    "\n",
    "People have investigated many variations of gradient descent, including variations that more closely mimic a real physical ball. These ball-mimicking variations have some advantages, but also have a major disadvantage: it turns out to be necessary to compute second partial derivatives of C, and this can be quite costly. To see why it's costly, suppose we want to compute all the second partial derivatives $\\partial^2 C/ \\partial v_j \\partial v_k$. If there are a million such vj variables then we'd need to compute something like a trillion (i.e., a million squared) second partial derivatives*  That's going to be computationally costly. With that said, there are tricks for avoiding this kind of problem, and finding alternatives to gradient descent is an active area of investigation. But in this book we'll use gradient descent (and variations) as our main approach to learning in neural networks.\n",
    "\n",
    "How can we apply gradient descent to learn in a neural network? The idea is to use gradient descent to find the weights wk and biases bl which minimize the cost in Equation (6). To see how this works, let's restate the gradient descent update rule, with the weights and biases replacing the variables vj. In other words, our \"position\" now has components $w_k$ and $b_l$, and the gradient vector ∇C has corresponding components $∂C/∂w_k$ and $∂C/∂b_l$. Writing out the gradient descent update rule in terms of components, we have \n",
    "\n",
    "$$ \\begin{eqnarray}\n",
    "  w_k & \\rightarrow & w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\tag{16}\\\\\n",
    "  b_l & \\rightarrow & b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l}.\n",
    "\\tag{17}\\end{eqnarray} $$\n",
    "\n",
    " By repeatedly applying this update rule we can \"roll down the hill\", and hopefully find a minimum of the cost function. In other words, this is a rule which can be used to learn in a neural network.\n",
    " \n",
    " There are a number of challenges in applying the gradient descent rule. We'll look into those in depth in later chapters. But for now I just want to mention one problem. To understand what the problem is, let's look back at the quadratic cost in Equation (6). Notice that this cost function has the form $C = \\frac{1}{n} \\sum_x C_x$, that is, it's an average over costs $C_x \\equiv \\frac{\\|y(x)-a\\|^2}{2}$ for individual training examples. In practice, to compute the gradient ∇C we need to compute the gradients ∇Cx separately for each training input, x, and then average them, $\\nabla C = \\frac{1}{n}\n",
    "\\sum_x \\nabla C_x$. **Unfortunately, when the number of training inputs is very large this can take a long time, and learning thus occurs slowly**.\n",
    "\n",
    "An idea called **stochastic gradient descent** can be used to speed up learning. The idea is to estimate the gradient ∇C by computing $∇C_x$ for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient ∇C, and this helps speed up gradient descent, and thus learning.\n",
    "\n",
    "To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number m of randomly chosen training inputs. We'll label those random training inputs $X_1,X_2,…,X_m,$ and refer to them as a mini-batch. Provided the sample size m is large enough we expect that the average value of the $∇CX_j$ will be roughly equal to the average over all $∇C_x$, that is, \n",
    "\n",
    "$$ \\begin{eqnarray}\n",
    "  \\frac{\\sum_{j=1}^m \\nabla C_{X_{j}}}{m} \\approx \\frac{\\sum_x \\nabla C_x}{n} = \\nabla C,\n",
    "\\tag{18}\\end{eqnarray} $$\n",
    "\n",
    " where the second sum is over the entire set of training data. Swapping sides we get \n",
    " \n",
    " $$\\begin{eqnarray}\n",
    "  \\nabla C \\approx \\frac{1}{m} \\sum_{j=1}^m \\nabla C_{X_{j}},\n",
    "\\tag{19}\\end{eqnarray}$$\n",
    "\n",
    " confirming that we can estimate the overall gradient by computing gradients just for the randomly chosen mini-batch. \n",
    " \n",
    " To connect this explicitly to learning in neural networks, suppose $w_k$ and $b_l$ denote the weights and biases in our neural network. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those, \n",
    " \n",
    " $$\\begin{eqnarray} \n",
    "  w_k & \\rightarrow & w_k' = w_k-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k} \\tag{20}\\\\ \n",
    "  b_l & \\rightarrow & b_l' = b_l-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l},\n",
    "\\tag{21}\\end{eqnarray}$$\n",
    "\n",
    " where the sums are over all the training examples $X_j$ in the current mini-batch. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we've exhausted the training inputs, which is said to complete an **epoch** of training. At that point we start over with a new training epoch.\n",
    " \n",
    " Incidentally, it's worth noting that conventions vary about scaling of the cost function and of mini-batch updates to the weights and biases. In Equation (6) we scaled the overall cost function by a factor 1n. People sometimes omit the 1n, summing over the costs of individual training examples instead of averaging. **This is particularly useful when the total number of training examples isn't known in advance.** This can occur if more training data is being generated in real time, for instance. And, in a similar way, the mini-batch update rules (20) and (21) sometimes omit the 1m term out the front of the sums. Conceptually this makes little difference, since it's equivalent to rescaling the learning rate η . But when doing detailed comparisons of different work it's worth watching out for.\n",
    "\n",
    "We can think of stochastic gradient descent as being like political polling: it's much easier to sample a small mini-batch than it is to apply gradient descent to the full batch, just as carrying out a poll is easier than running a full election. For example, if we have a training set of size n=60,000\n",
    ", as in MNIST, and choose a mini-batch size of (say) m=10, this means we'll get a factor of 6,000 speedup in estimating the gradient! Of course, the estimate won't be perfect - there will be statistical fluctuations - but it doesn't need to be perfect: all we really care about is moving in a general direction that will help decrease C, and that means we don't need an exact computation of the gradient. In practice, stochastic gradient descent is a commonly used and powerful technique for learning in neural networks, and it's the basis for most of the learning techniques we'll develop in this book.\n",
    "\n",
    "## How the backpropagation algorithm works\n",
    "\n",
    "The backpropagation algorithm was originally introduced in the 1970s, but its importance wasn't fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. That paper describes several neural networks where backpropagation works far faster than earlier approaches to learning, making it possible to use neural nets to solve problems which had previously been insoluble. Today, the backpropagation algorithm is the workhorse of learning in neural networks.\n",
    "\n",
    "At the heart of backpropagation is an expression for the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases. And while the expression is somewhat complex, it also has a beauty to it, with each element having a natural, intuitive interpretation. And so backpropagation isn't just a fast algorithm for learning. It actually gives us detailed insights into how changing the weights and biases changes the overall behaviour of the network. That's well worth studying in detail.\n",
    "\n",
    "### The two assumptions we need about the cost function\n",
    "The goal of backpropagation is to compute the partial derivatives ∂C/∂w and ∂C/∂b of the cost function C with respect to any weight w or bias b in the network. For backpropagation to work we need to make two main assumptions about the form of the cost function. Before stating those assumptions, though, it's useful to have an example cost function in mind. We'll use the quadratic cost function from last chapter (c.f. Equation (6)). In the notation of the last section, the quadratic cost has the form\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  C = \\frac{1}{2n} \\sum_x \\|y(x)-a^L(x)\\|^2,\n",
    "\\tag{26}\\end{eqnarray}\n",
    "\n",
    " where: n is the total number of training examples; the sum is over individual training examples, x; y=y(x) is the corresponding desired output; L denotes the number of layers in the network; and $a^L = a^L(x)$ is the vector of activations output from the network when x is input.\n",
    "\n",
    "Okay, so what assumptions do we need to make about our cost function, C, in order that backpropagation can be applied? The first assumption we need is that the cost function can be written as an average $C = \\frac{1}{n} \\sum_x C_x$ over cost functions Cx for individual training examples, x. This is the case for the quadratic cost function, where the cost for a single training example is $C_x =\n",
    "\\frac{1}{2} \\|y-a^L \\|^2$. This assumption will also hold true for all the other cost functions we'll meet in this book.\n",
    "\n",
    "The reason we need this assumption is because what backpropagation actually lets us do is compute the partial derivatives $∂C_x/∂w$ and $∂C_x/∂b$ for a single training example. We then recover ∂C/∂w and ∂C/∂b by averaging over training examples. In fact, with this assumption in mind, we'll suppose the training example x has been fixed, and drop the x subscript, writing the cost $C_x$ as C. We'll eventually put the x back in, but for now it's a notational nuisance that is better left implicit.\n",
    "\n",
    "The second assumption we make about the cost is that it can be written as a function of the outputs from the neural network: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz18.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, the quadratic cost function satisfies this requirement, since the quadratic cost for a single training example x may be written as \n",
    " \n",
    " \\begin{eqnarray}\n",
    "  C = \\frac{1}{2} \\|y-a^L\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2,\n",
    "\\tag{27}\\end{eqnarray}\n",
    "\n",
    "and thus is a function of the output activations. Of course, this cost function also depends on the desired output y, and you may wonder why we're not regarding the cost also as a function of y. Remember, though, that the input training example x is fixed, and so the output y is also a fixed parameter. In particular, it's not something we can modify by changing the weights and biases in any way, i.e., it's not something which the neural network learns. And so it makes sense to regard C as a function of the output activations aL alone, with y merely a parameter that helps define that function.\n",
    "\n",
    "### The Hadamard product, s⊙t\n",
    "The backpropagation algorithm is based on common linear algebraic operations - things like vector addition, multiplying a vector by a matrix, and so on. But one of the operations is a little less commonly used. In particular, suppose s and t are two vectors of the same dimension. Then we use s⊙t to denote the elementwise product of the two vectors. Thus the components of s⊙t are just (s⊙t)j=sjtj. As an example, \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \n",
    "  \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\n",
    "\\tag{28}\\end{eqnarray}\n",
    "\n",
    " This kind of elementwise multiplication is sometimes called the Hadamard product or Schur product. We'll refer to it as the Hadamard product. Good matrix libraries usually provide fast implementations of the Hadamard product, and that comes in handy when implementing backpropagation.\n",
    " \n",
    " ### The four fundamental equations behind backpropagation\n",
    " Backpropagation is about understanding how changing the weights and biases in a network changes the cost function. Ultimately, this means computing the partial derivatives $\\partial C / \\partial w^l_{jk}$ and $\\partial C / \\partial b^l_j$. But to compute those, we first introduce an intermediate quantity, $δ_j^l$, which we call the error in the $j^{\\rm th}$ neuron in the $l^{\\rm th}$ layer. Backpropagation will give us a procedure to compute the error $δl_j$, and then will relate $δl_j$ to $\\partial C\n",
    "/ \\partial w^l_{jk}$ and $\\partial C / \\partial b^l_j$.\n",
    "\n",
    "To understand how the error is defined, imagine there is a demon in our neural network: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz19.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demon sits at the jth neuron in layer l. As the input to the neuron comes in, the demon messes with the neuron's operation. It adds a little change $Δzl_j$ to the neuron's weighted input, so that instead of outputting $σ(zl_j)$, the neuron instead outputs $\\sigma(z^l_j+\\Delta z^l_j)$. This change propagates through later layers in the network, finally causing the overall cost to change by an amount $\\frac{\\partial C}{\\partial z^l_j} \\Delta z^l_j$.\n",
    "\n",
    "Now, this demon is a good demon, and is trying to help you improve the cost, i.e., they're trying to find a $Δzl_j$\n",
    "which makes the cost smaller. Suppose $\\frac{\\partial C}{\\partial z^l_j}$ has a large value (either positive or negative). Then the demon can lower the cost quite a bit by choosing $Δzl_j$ to have the opposite sign to $\\frac{\\partial C}{\\partial z^l_j}$. By contrast, if $\\frac{\\partial C}{\\partial z^l_j}$ is close to zero, then the demon can't improve the cost much at all by perturbing the weighted input $zl_j$. So far as the demon can tell, the neuron is already pretty near optimal* And so there's a heuristic sense in which $\\frac{\\partial C}{\\partial z^l_j}$ is a measure of the error in the neuron.\n",
    "\n",
    "Motivated by this story, we define the error $δl_j$ of neuron j in layer l by \n",
    "\\begin{eqnarray} \n",
    "  \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}.\n",
    "\\tag{29}\\end{eqnarray}\n",
    "\n",
    "**Plan of attack**: Backpropagation is based around four fundamental equations. Together, those equations give us a way of computing both the error δl and the gradient of the cost function. I state the four equations below. Be warned, though: you shouldn't expect to instantaneously assimilate the equations. Such an expectation will lead to disappointment. In fact, the backpropagation equations are so rich that understanding them well requires considerable time and patience as you gradually delve deeper into the equations. The good news is that such patience is repaid many times over. And so the discussion in this section is merely a beginning, helping you on the way to a through understanding of the equations.\n",
    "\n",
    "An equation for the error in the output layer, $δ^L$: The components of $δ^L$ are given by \n",
    "\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\tag{BP1}\\end{eqnarray}\n",
    "\n",
    "This is a very natural expression. The first term on the right, $∂C/∂a^L_j$, just measures how fast the cost is changing as a function of the jth output activation. If, for example, C doesn't depend much on a particular output neuron, j, then $δ^L_j$ will be small, which is what we'd expect. The second term on the right, $σ′(z^L_j$), measures how fast the activation function σ is changing at $z^L_j$.\n",
    "\n",
    "Notice that everything in (BP1) is easily computed. In particular, we compute $z^L_j$\n",
    "while computing the behaviour of the network, and it's only a small additional overhead to compute $σ′(z^L_j)$. The exact form of $∂C/∂a^L_j$ will, of course, depend on the form of the cost function. However, provided the cost function is known there should be little trouble computing $∂C/∂a^L_j$. For example, if we're using the quadratic cost function then $C = \\frac{1}{2} \\sum_j\n",
    "(y_j-a^L_j)^2$, and so $\\partial C / \\partial a^L_j = (a_j^L-y_j)$, which obviously is easily computable.\n",
    "\n",
    "Equation (BP1) is a componentwise expression for $δ^L$\n",
    ". It's a perfectly good expression, but not the matrix-based form we want for backpropagation. However, it's easy to rewrite the equation in a matrix-based form, as \n",
    "\\begin{eqnarray} \n",
    "  \\delta^L = \\nabla_a C \\odot \\sigma'(z^L).\n",
    "\\tag{BP1a}\\end{eqnarray}\n",
    "\n",
    " Here, ∇aC is defined to be a vector whose components are the partial derivatives $∂C/∂a^L_j$. You can think of ∇aC as expressing the rate of change of C with respect to the output activations. It's easy to see that Equations (BP1a) and (BP1) are equivalent, and for that reason from now on we'll use (BP1) interchangeably to refer to both equations. As an example, in the case of the quadratic cost we have ∇aC=(aL−y), and so the fully matrix-based form of (BP1) becomes \n",
    " \n",
    " \\begin{eqnarray} \n",
    "  \\delta^L = (a^L-y) \\odot \\sigma'(z^L).\n",
    "\\tag{30}\\end{eqnarray}\n",
    "\n",
    " As you can see, everything in this expression has a nice vector form, and is easily computed using a library such as Numpy.\n",
    "\n",
    "**An equation for the error $δ^l$ in terms of the error in the next layer**, δl+1: In particular \n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "\\tag{BP2}\\end{eqnarray}\n",
    "\n",
    " where (wl+1)T is the transpose of the weight matrix wl+1 for the (l+1)th layer. This equation appears complicated, but each element has a nice interpretation. Suppose we know the error δl+1 at the l+1th layer. When we apply the transpose weight matrix, (wl+1)T, we can think intuitively of this as moving the error backward through the network, giving us some sort of measure of the error at the output of the lth layer. We then take the Hadamard product ⊙σ′(zl). This moves the error backward through the activation function in layer l, giving us the error δl in the weighted input to layer l.\n",
    "\n",
    "By combining (BP2) with (BP1) we can compute the error $δ_l$\n",
    "for any layer in the network. We start by using (BP1) to compute $δ^L$, then apply Equation (BP2) to compute $δ^{L−1}$, then Equation (BP2) again to compute $δ^{L−2}$, and so on, all the way back through the network.\n",
    "\n",
    "**An equation for the rate of change of the cost with respect to any bias in the network:** In particular: \n",
    "\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j.\n",
    "\\tag{BP3}\\end{eqnarray}\n",
    "\n",
    " That is, the error $δl_j$ is exactly equal to the rate of change $∂C/∂b^l_j$. This is great news, since (BP1) and (BP2) have already told us how to compute $δl_j$. We can rewrite (BP3) in shorthand as \n",
    " \\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial b} = \\delta,\n",
    "\\tag{31}\\end{eqnarray}\n",
    " where it is understood that δ is being evaluated at the same neuron as the bias b.\n",
    "\n",
    "**An equation for the rate of change of the cost with respect to any weight in the network:** In particular: \n",
    "\n",
    "\\begin{eqnarray}  \n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\tag{BP4}\\end{eqnarray}\n",
    "\n",
    " This tells us how to compute the partial derivatives $∂C/∂w_lj_k$ in terms of the quantities $δ_l$ and $a_{l−1}$, which we already know how to compute. The equation can be rewritten in a less index-heavy notation as \n",
    " \\begin{eqnarray}  \\frac{\\partial\n",
    "    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out},\n",
    "\\tag{32}\\end{eqnarray}\n",
    " where it's understood that ain is the activation of the neuron input to the weight w, and δout is the error of the neuron output from the weight w. Zooming in to look at just the weight w, and the two neurons connected by that weight, we can depict this as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/tikz20.png\" alt=\"Drawing\" style=\"width:200px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A nice consequence of Equation (32) is that when the activation ain is small, ain≈0, the gradient term ∂C/∂w will also tend to be small. In this case, we'll say the weight learns slowly, meaning that it's not changing much during gradient descent. In other words, one consequence of (BP4) is that weights output from low-activation neurons learn slowly.\n",
    "\n",
    "There are other insights along these lines which can be obtained from (BP1)-(BP4). Let's start by looking at the output layer. Consider the term $σ′(z^L_j)$\n",
    "in (BP1). Recall from the graph of the sigmoid function in the last chapter that the σ function becomes very flat when $σ(z^L_j)$ is approximately 0 or 1. When this occurs we will have $σ′(z^L_j)≈0$. And so the lesson is that a weight in the final layer will learn slowly if the output neuron is either low activation (≈0) or high activation (≈1). In this case it's common to say the output neuron has saturated and, as a result, the weight has stopped learning (or is learning slowly). Similar remarks hold also for the biases of output neuron.\n",
    "\n",
    "We can obtain similar insights for earlier layers. In particular, note the $σ′(z_l)$ term in (BP2). This means that δlj is likely to get small if the neuron is near saturation. And this, in turn, means that any weights input to a saturated neuron will learn slowly* *This reasoning won't hold if $w_l+1Tδ_l+1$ has large enough entries to compensate for the smallness of $σ′(zl_j)$. But I'm speaking of the general tendency..\n",
    "\n",
    "Summing up, we've learnt that a weight will learn slowly if either the input neuron is low-activation, or if the output neuron has saturated, i.e., is either high- or low-activation.\n",
    "\n",
    "None of these observations is too greatly surprising. Still, they help improve our mental model of what's going on as a neural network learns. Furthermore, we can turn this type of reasoning around. The four fundamental equations turn out to hold for any activation function, not just the standard sigmoid function (that's because, as we'll see in a moment, the proofs don't use any special properties of σ\n",
    "). And so we can use these equations to design activation functions which have particular desired learning properties. As an example to give you the idea, suppose we were to choose a (non-sigmoid) activation function σ so that σ′ is always positive, and never gets close to zero. That would prevent the slow-down of learning that occurs when ordinary sigmoid neurons saturate. Later in the book we'll see examples where this kind of modification is made to the activation function. Keeping the four equations (BP1)-(BP4) in mind can help explain why such modifications are tried, and what impact they can have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/Selection_218.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The backpropagation algorithm\n",
    "The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/Selection_219.png\" alt=\"Drawing\" style=\"width:400px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the algorithm you can see why it's called **backpropagation**. We compute the error vectors $δ_l$ backward, starting from the final layer. It may seem peculiar that we're going through the network backward. But if you think about the proof of backpropagation, the backward movement is a consequence of the fact that the cost is a function of outputs from the network. To understand how the cost varies with earlier weights and biases we need to repeatedly apply the chain rule, working backward through the layers to obtain usable expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More about Neural Networks\n",
    "\n",
    "[Improving the way neural networks learn - Overfitting and regularization](http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization)\n",
    "\n",
    "[A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
